---
# yamllint disable rule:truthy rule:line-length rule:comments
permissions:
  contents: read

name: Hybrid CI
concurrency:
  group: ci-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

"on":
  push:
    branches: [main]
    paths:
      - ".github/workflows/ci.yml"
      - "docker/**"
      - "scripts/**"
      - ".devcontainer/**"
      - "pixi.toml"
      - "pixi.lock"
      - "pyproject.toml"
  schedule:
    - cron: "0 4 * * 1"
  workflow_dispatch:

jobs:
  quality:
    env:
      PIXI_PLATFORM: linux-64
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          persist-credentials: false
      - name: Set artifacts dir
        run: echo "ARTIFACTS_DIR=$RUNNER_TEMP/artifacts" >> "$GITHUB_ENV"
      - name: Prepare artifacts directory
        run: mkdir -p "$ARTIFACTS_DIR"
      - uses: prefix-dev/setup-pixi@82d477f15f3a381dbcc8adc1206ce643fe110fb7 # v0.9.3
        with:
          cache: true
          locked: true
          environments: automation
      - name: Collect pixi metadata
        run: |
          mkdir -p "$ARTIFACTS_DIR/pixi"
          cp pixi.toml pixi.lock "$ARTIFACTS_DIR/pixi/"
          set +e
          pixi info --json > "$ARTIFACTS_DIR/pixi/info.json" || echo '{"note":"pixi info unavailable"}' > "$ARTIFACTS_DIR/pixi/info.json"
          pixi list --json > "$ARTIFACTS_DIR/pixi/list.json" || echo '{"note":"pixi list unavailable"}' > "$ARTIFACTS_DIR/pixi/list.json"
          pixi tree -e automation > "$ARTIFACTS_DIR/pixi/tree.txt" || echo "pixi tree unavailable" > "$ARTIFACTS_DIR/pixi/tree.txt"
          echo '{"note":"pixi manifest/diff/updates_github_actions CLI not available in pixi 0.62.2; captured manifest + lock instead."}' > "$ARTIFACTS_DIR/pixi/notes.json"
          set -e
      - name: Hadolint (Dockerfile)
        uses: hadolint/hadolint-action@2332a7b74a6de0dda2e2221d575162eba76ba5e5 # v3.3.0
        with:
          dockerfile: docker/Dockerfile
      # Runs the full Strict Suite
      - name: Run validate (with reports)
        env:
          RUN_SEMGREP: "1"
          PYTEST_ADDOPTS: "--cov-report=xml:${ARTIFACTS_DIR}/coverage.xml --junitxml=${ARTIFACTS_DIR}/tests.xml"
        run: |
          set -o pipefail
          pixi run -e automation validate | tee "$ARTIFACTS_DIR/validate.log"
      - name: Generate validation summary
        if: always()
        run: |
          python - <<'PY'
          import json
          import os
          import xml.etree.ElementTree as ET
          from pathlib import Path

          artifacts = Path(os.environ["ARTIFACTS_DIR"])
          summary = {"coverage": {}, "tests": {}}

          cov_xml = artifacts / "coverage.xml"
          if cov_xml.exists():
              root = ET.parse(cov_xml).getroot()
              summary["coverage"] = {
                  "line_rate": root.get("line-rate"),
                  "branch_rate": root.get("branch-rate"),
                  "lines_valid": root.get("lines-valid"),
                  "lines_covered": root.get("lines-covered"),
              }

          junit_xml = artifacts / "tests.xml"
          if junit_xml.exists():
              suite = ET.parse(junit_xml).getroot()
              summary["tests"] = {
                  "tests": suite.get("tests"),
                  "failures": suite.get("failures"),
                  "errors": suite.get("errors"),
                  "skipped": suite.get("skipped"),
              }

          artifacts.mkdir(parents=True, exist_ok=True)
          (artifacts / "quality-summary.json").write_text(
              json.dumps(summary, indent=2),
              encoding="utf-8",
          )
          with (artifacts / "quality-summary.md").open("w", encoding="utf-8") as fh:
              fh.write("| metric | value |\\n|---|---|\\n")
              for key, value in summary.get("coverage", {}).items():
                  fh.write(f"| coverage:{key} | {value or 'n/a'} |\\n")
              for key, value in summary.get("tests", {}).items():
                  fh.write(f"| tests:{key} | {value or 'n/a'} |\\n")
          PY
      - name: Append quality summary to job summary
        if: always()
        run: |
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          summary_path = Path(os.environ["GITHUB_STEP_SUMMARY"])
          artifacts = Path(os.environ["ARTIFACTS_DIR"])
          data = {}
          try:
              data = json.loads((artifacts / "quality-summary.json").read_text(encoding="utf-8"))
          except FileNotFoundError:
              pass

          cov = data.get("coverage", {})
          tests = data.get("tests", {})
          lines = [
              "## Quality Results",
              "| metric | value |",
              "|---|---|",
              f"| coverage:line_rate | {cov.get('line_rate', 'n/a')} |",
              f"| coverage:branch_rate | {cov.get('branch_rate', 'n/a')} |",
              f"| coverage:lines_valid | {cov.get('lines_valid', 'n/a')} |",
              f"| coverage:lines_covered | {cov.get('lines_covered', 'n/a')} |",
              f"| tests:total | {tests.get('tests', 'n/a')} |",
              f"| tests:failures | {tests.get('failures', 'n/a')} |",
              f"| tests:errors | {tests.get('errors', 'n/a')} |",
              f"| tests:skipped | {tests.get('skipped', 'n/a')} |",
              "",
              f"[Artifacts directory]({{ env.ARTIFACTS_DIR }})",
          ]
          with summary_path.open("a", encoding="utf-8") as handle:
              handle.write("\n".join(lines) + "\n")
          PY
      - name: Ensure working tree clean (no generated diffs)
        run: git diff --exit-code
      - name: Upload quality artifacts
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: quality-${{ github.run_id }}
          path: ${{ env.ARTIFACTS_DIR }}/**
          if-no-files-found: warn

  build:
    needs: quality
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        os: [focal, noble]
        env: [stable]
    permissions:
      contents: read
      packages: write
      id-token: write
      attestations: write
    env:
      PIXI_PLATFORM: linux-64
      REGISTRY: ghcr.io/ray-manaloto/pixi-devcontainer
      BUILD_OS: ${{ matrix.os }}
      BUILD_ENV: ${{ matrix.env }}
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          persist-credentials: false
      - name: Set artifacts dir
        run: echo "ARTIFACTS_DIR=$RUNNER_TEMP/artifacts-${{ matrix.os }}-${{ matrix.env }}" >> "$GITHUB_ENV"
      - name: Prepare artifacts directory
        run: mkdir -p "$ARTIFACTS_DIR" build
      - uses: prefix-dev/setup-pixi@82d477f15f3a381dbcc8adc1206ce643fe110fb7 # v0.9.3
        with:
          cache: true
          locked: true
          environments: automation
      - uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3.12.0
        with:
          driver-opts: |
            image=moby/buildkit:latest
            network=host
            env.BUILDKIT_STEP_LOG_MAX_SIZE=10485760
            env.BUILDKIT_STEP_LOG_MAX_SPEED=10485760
      - name: Capture buildx info
        run: docker buildx inspect > "$ARTIFACTS_DIR/buildx-info.txt"
      - name: Login to GHCR
        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT != '' && secrets.GHCR_PAT || secrets.GITHUB_TOKEN }}
      - name: Bake plan
        run: |
          docker buildx bake -f docker/docker-bake.hcl \
            image-${{ matrix.os }}-${{ matrix.env }} \
            artifact-${{ matrix.os }}-${{ matrix.env }} \
            --print > "$ARTIFACTS_DIR/bake-plan.json"
      - name: Build devcontainer images
        id: build
        run: |
          set -o pipefail
          BUILD_TARGETS="image-${BUILD_OS}-${BUILD_ENV} artifact-${BUILD_OS}-${BUILD_ENV}" \
          pixi run -e automation build | tee "$ARTIFACTS_DIR/build.log"
      - name: Export config hash for downstream steps
        if: always()
        env:
          HASH: ${{ steps.build.outputs.HASH || 'unknown' }}
        run: printf 'CONFIG_HASH=%s\n' "$HASH" >> "$GITHUB_ENV"
      - name: Summarize OCI artifacts
        env:
          REGISTRY: ${{ env.REGISTRY }}
        run: |
          python - <<'PY'
          import json
          from pathlib import Path
          import os

          config_hash = os.environ.get("CONFIG_HASH", "unknown")
          registry = os.environ.get("REGISTRY", "ghcr.io/ray-manaloto/pixi-devcontainer")
          artifacts = []
          for tar in Path("build").glob("*.tar"):
              stat = tar.stat()
              artifacts.append(
                  {
                      "file": tar.name,
                      "size_bytes": stat.st_size,
                      "modified": stat.st_mtime,
                  },
              )
          os_name = os.environ.get("BUILD_OS", "focal")
          env_name = os.environ.get("BUILD_ENV", "stable")
          planned = [
              {
                  "image": f"{registry}:{os_name}-{env_name}-{config_hash}",
                  "latest": f"{registry}:{os_name}-{env_name}-latest",
                  "os": os_name,
                  "env": env_name,
              },
          ]
          out_dir = Path(os.environ["ARTIFACTS_DIR"])
          out_dir.mkdir(parents=True, exist_ok=True)
          (out_dir / "oci-metadata.json").write_text(
              json.dumps({"images": artifacts, "planned_tags": planned}, indent=2),
              encoding="utf-8",
          )
          PY
      - name: Capture GHCR upload status
        env:
          REGISTRY: ${{ env.REGISTRY }}
        run: |
          python - <<'PY'
          import datetime as dt
          import json
          import os
          import subprocess
          from pathlib import Path

          registry = os.environ.get("REGISTRY", "ghcr.io/ray-manaloto/pixi-devcontainer")
          config_hash = os.environ.get("CONFIG_HASH", "unknown")
          out_dir = Path(os.environ["ARTIFACTS_DIR"])
          out_dir.mkdir(parents=True, exist_ok=True)

          records = []
          package_url = "https://github.com/users/ray-manaloto/packages/container/pixi-devcontainer"
          os_name = os.environ.get("BUILD_OS", "focal")
          env_name = os.environ.get("BUILD_ENV", "stable")
          for suffix in (config_hash, "latest"):
              tag = f"{registry}:{os_name}-{env_name}-{suffix}"
              entry: dict[str, object] = {
                  "image": tag,
                  "os": os_name,
                      "env": env_name,
                      "tag_suffix": suffix,
                      "config_hash": config_hash,
                      "attestations": ["provenance", "sbom"],
                      "checked_at": dt.datetime.utcnow().isoformat(timespec="seconds") + "Z",
                  }
                  try:
                      raw = subprocess.check_output(  # noqa: S603
                          ["docker", "buildx", "imagetools", "inspect", "--raw", tag],
                          text=True,
                      )
                      data = json.loads(raw)
                      manifests = data.get("manifests") or []
                      platforms = []
                      sizes = []
                      digest = None
                      for manifest in manifests:
                          platform = manifest.get("platform") or {}
                          if platform:
                              platforms.append(
                                  f"{platform.get('os', '?')}/{platform.get('architecture', '?')}",
                              )
                          if manifest.get("size"):
                              sizes.append(manifest["size"])
                          if manifest.get("digest") and not digest:
                              digest = manifest["digest"]
                      entry.update(
                          {
                              "status": "available",
                              "digest": digest or data.get("config", {}).get("digest"),
                              "platforms": platforms,
                              "size_bytes_total": sum(sizes) if sizes else None,
                          },
                      )
                  except subprocess.CalledProcessError as exc:  # noqa: PERF203
                      entry.update({"status": "inspect_failed", "error": str(exc)})
                  records.append(entry)

          payload = {"images": records}
          (out_dir / "ghcr-status.json").write_text(
              json.dumps(payload, indent=2),
              encoding="utf-8",
          )
          lines = ["| image | status | digest | platforms | size_bytes |", "|---|---|---|---|---|"]
          for rec in records:
              image_link = f"[{rec['image']}]({package_url})"
              lines.append(
                  f"| {image_link} | {rec.get('status')} | {rec.get('digest') or 'n/a'} | "
                  f"{', '.join(rec.get('platforms') or []) or 'n/a'} | "
                  f"{rec.get('size_bytes_total') or 'n/a'} |",
              )
          (out_dir / "ghcr-status.md").write_text("\n".join(lines), encoding="utf-8")
          PY
      - name: Append build summary to job summary
        if: always()
        env:
          REGISTRY: ${{ env.REGISTRY }}
        run: |
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          summary_path = Path(os.environ["GITHUB_STEP_SUMMARY"])
          artifacts = Path(os.environ["ARTIFACTS_DIR"])
          registry = os.environ.get("REGISTRY", "ghcr.io/ray-manaloto/pixi-devcontainer")

          lines = ["## Build Summary"]

          # Bake targets
          try:
              plan = json.loads((artifacts / "bake-plan.json").read_text(encoding="utf-8"))
              targets = sorted(plan.get("target", {}).keys())
              if targets:
                  lines.append("| bake targets |")
                  lines.append("|---|")
                  for t in targets:
                      lines.append(f"| {t} |")
                  lines.append("")
          except FileNotFoundError:
              lines.append("_bake plan missing_")

          # GHCR status table (already pre-rendered)
          status_md = artifacts / "ghcr-status.md"
          if status_md.exists():
              lines.append("### GHCR status")
              lines.append(status_md.read_text(encoding="utf-8"))
              lines.append("")
          else:
              lines.append("_ghcr status missing_")

          # OCI artifacts
          try:
              oci = json.loads((artifacts / "oci-metadata.json").read_text(encoding="utf-8"))
              lines.append("### OCI artifacts")
              images = oci.get("images", [])
              if images:
                  lines.append("| file | size_bytes |")
                  lines.append("|---|---|")
                  for img in images:
                      lines.append(f"| {img.get('file')} | {img.get('size_bytes')} |")
                  lines.append("")
              else:
                  lines.append("_no local OCI tarballs (push-only build)_")
                  lines.append("")
          except FileNotFoundError:
              lines.append("_oci metadata missing_")

          lines.append(f"[Build log]({{ env.ARTIFACTS_DIR }}/build.log)")
          lines.append(f"[Bake plan]({{ env.ARTIFACTS_DIR }}/bake-plan.json)")
          lines.append(f"[Buildx info]({{ env.ARTIFACTS_DIR }}/buildx-info.txt)")

          with summary_path.open("a", encoding="utf-8") as handle:
              handle.write("\n".join(lines) + "\n")
          PY
      - name: Upload devcontainer OCI artifacts
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: devcontainer-build-${{ steps.build.outputs.HASH || github.run_id }}
          path: |
            ${{ env.ARTIFACTS_DIR }}/**
            build/*.tar
            dist/**
          if-no-files-found: error
